TF-IDF (Term Frequency - Inverse Document Frequency) - 텍스트들의 유사도 거리
TF ( 단어 빈도 )는 특정한 단어가 문서내에 얼마나 자주 등장하는지를 나타내는 값이다.
이 값이 높을 수록 문서에서 중요하다고 생각할 수 있다.
하지만 하나의 문서에서 많이 나오지 않고 다른 문서에서 자주 등장하면 단어의 중요도는 낮아진다.

DF (문서 빈도) 이 값의 역수를 IDF(역문서빈도)
TF-IDF는 TF와 IDF를 곱한 값으로 점수가 높은 단어 일수록 다른 문서에는 많지 않고 해당 문서에서 자주 등장하는 단어의미이다.

예)

문서 1 : if you think you can
문서 2 : or you think you can not you are right

TF-IDF 계산 단계

1. 각 문서에 대한 각 단어의 빈도를 계산 (TF)
2. IDF 계산
3. TF*IDF

문서 1 
단어   단어수
------------
if      1
you     2
think   1
can     1

문서 2
단어   단어수
------------
or      1
you     3
think   1
can     1
not     1
are     1
right   1

문서 1 단어 수 : 5개
문서 2 단어 수 : 9개

단계 1) 
        어떤 문서에서 단어t가 나오는 횟수
TF(t) = ------------------------------
            그 문서에 있는 단어의 총수 
            
            
TF('think',문서1) = 1/5 = 0.2
TF('think',문서2) = 1/9 ≒ 0.11

단계 2)

    IDF(t) = log10(전체 문서의 수 / 단어t가 들어간 문서의 수) (상용로그 밑 10)
import numpy as np
    IDF('think',D) = np.log10(2/2) = 0

단계 3)
TF * IDF 계산
TFIDF('think',문서1) = 0.2 * 0 = 0   
TFIDF('think',문서2) = 0.11 * 0 = 0
    

단어가 right인 경우 

TF ('right',문서2) = 1/9

IDF('right',문서) = np.log10(2/1) = 0.3010

TFIDF('right',문서) = np.log10(2) * 1/9 = 0.03

word2vec(word to vector)
- 구글의 토마스미콜로프(Tomas Mikolov) 이끄는 팀이 개발
- 2계층 신경망 (two layer neural network)을 사용해 개발
- 텍스트에서 벡터집합을 생성한다.

from konlpy.tag import Komoran
tagger = Komoran 
from urllib.parse import quote_plus
from bs4 import BeautifulSoup
import requests
import lxml.html            
import codecs

fp = codecs.open('c:/data/미술관옆동물원.txt','r')
soup = BeautifulSoup(fp,'html.parser')
body = soup.select_one('body')
text = body.getText()
articles = []
articles = text.split('\n')
len(articles)
fp.close()

from sklearn.feature_extraction.text import TfidfVectorizer

TfidfVectorizer : TF-IDF 방정식으로 단어의 가중치를 조정한 BOW 벡터를 만든다.
BOW (Bag Of Words) : 문서를 숫자 벡터로 변환하는 가장 기본적인 방법

corpus = ['This is the first document','This document is the second document','And this is the third one','is this the first document']
v = TfidfVectorizer()
x = v.fit_transform(corpus) # Term document Matrix 생성
print(v.get_feature_names()) # 알파벳 순으로 나온다.
x.shape
Out[42]: (4, 9) # (문장의 수, 단어의 수 )
print(x) # v.get_feature_names() 여기단어들의 인덱스를 해놔서 값을 뽑아낸다.

print(v.vocabulary_.get('first'))
print(v.vocabulary_.get('document'))

from sklearn.feature_extraction.text import CountVectorizer
- 문서를 토큰리스트로 변환한다.
- 각 문서에서 토큰의 출현빈도를 센다.
- 각 문서를 BOW 인코딩 벡터로 변환한다.

c = CountVectorizer()
c.fit(corpus)
c.vocabulary_ # 빈도수 체크값이 나온다.
c.transform(['This is the second document']).toarray() # 원래 있는 단어에서 문장을 바라본다.
# print(v.get_feature_names()) 이순서의 인덱스 값으로 문장안의 단어가 존재하는지 보여준다.
#['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']

c.transform(corpus).toarray() # 문장별로 단어가 있는지 없는지를 체크한다.

def get_noun(text):
    nouns = tagger.nouns(text)
    return [n for n in nouns is len(n) > 1]

cv = TfidfVectorizer(tokenizer=get_noun,max_features=100) # 왜 오류지?! article에 Null값이 있기 때문에 불가하다. Null값을 없애준다.
tdm = cv.fit_transform(articles_1)

pd.Series(articles)=='' # 이값이 있어서 되지 않는다.


cv = TfidfVectorizer(max_features=100) # 100개의 단어만추출
tdm = cv.fit_transform(articles)

import pandas as pd

count=0
articles_1=[]
for i in articles:
    if len(i)==0:
        print(count)
        continue
    articles_1.append(i)



cv = TfidfVectorizer(tokenizer=get_noun,max_features=100) # 왜 오류지?! article에 Null값이 있기 때문에 불가하다. Null값을 없애준다.
tdm = cv.fit_transform(articles_1)
print(cv.get_feature_names())
print(tdm.toarray())
print(tdm)

'''
articles = [article for article in articles if len(article)>0]

def get_noun(text):
    nouns=tagger.nouns(text)
    return [n for n in nouns if len(n)>1]

cv = TfidfVectorizer(tokenizer=get_noun, max_features=100)
tdm = cv.fit_transform(articles)
'''

import numpy as np
import operator 
words = cv.get_feature_names()
count_map = tdm.sum(axis=0)
count_map.shape
count = np.squeeze(np.array(count_map)) # squeeze: 1차원 없애는 함수
count.shape
word_count = list(zip(words,count))
word_count = sorted(word_count,key=operator.itemgetter(1),reverse=True)

hot_key = list(dict(word_count[:50]).keys())

from matplotlib import pyplot
from wordcloud import WordCloud
wc = WordCloud(font_path='c:/Windows/Fonts/batang.ttc',background_color='white',width=400,height=300)
cloud = wc.fit_words(dict(word_count))
pyplot.figure(figsize=(12,9))
pyplot.imshow(cloud)
pyplot.axis('off')
pyplot.show()


#### 선생님 파일 ###

from konlpy.tag import Komoran
tagger = Komoran()  # 형태소 분석기
from urllib.parse import quote_plus
from bs4 import BeautifulSoup
import requests
import lxml.html
import codecs


articles = []
fp = codecs.open("c:/data/미술관옆동물원.txt", "r")
soup = BeautifulSoup(fp, "html.parser")
body = soup.select_one("body")
text = body.getText()
articles = text.split("\n")
len(articles)
fp.close()



from sklearn.feature_extraction.text import TfidfVectorizer

def get_noun(text):
    nouns = tagger.nouns(text)
    return [n for n in nouns if len(n) > 1] 

del articles[0]
del articles[-1]
cv = TfidfVectorizer(tokenizer=get_noun, max_features=100)
tdm = cv.fit_transform(articles)
print(cv.get_feature_names())
print(tdm.toarray())
print(tdm) 


import numpy as np
import operator
words = cv.get_feature_names()

count_mat = tdm.sum(axis=0)
count_mat.shape
count = np.squeeze(np.asarray(count_mat))
count.shape
word_count = list(zip(words, count))
word_count = sorted(word_count, key=operator.itemgetter(1), reverse=True)
word_count

hot_key = list(dict(word_count[:50]).keys())
hot_key


%matplotlib inline
from matplotlib import pyplot
from wordcloud import WordCloud
wc = WordCloud(font_path='c:/Windows/Fonts/batang.ttc', background_color='white', width=400, height=300)
cloud = wc.fit_words(dict(word_count))
pyplot.figure(figsize=(12, 9))
pyplot.imshow(cloud)
pyplot.axis("off")
pyplot.show()

import codecs
from konlpy.tag import Twitter
from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.preprocessing import StandardScaler

def sigmoid(x):
    return 1 / (1 + math.e ** -x)


twitter = Twitter()
results = []
lines = articles
words_all = []

for line in lines:
    # 형태소 분석하기
    malist = twitter.pos(line, norm=True, stem=True)
    r = []
    for word in malist:
        # 명사/동사/부사만 걸러내기 
        if word[1] in ['Noun','Verb','Adjective']:
            r.append(word[0])
            words_all.append(word[0])
    rl = (" ".join(r)).strip()
    results.append(rl)
    #print(rl)

# pip install gensim

from gensim.models import word2vec
yang_file = 'c:/data/yang.model'
with open(yang_file, 'w', encoding='utf-8') as fp2:
    fp2.write("\n".join(results))
    
fp2.close() 

data = word2vec.LineSentence(yang_file)
model = word2vec.Word2Vec(data,size=200, window=10, hs=1, min_count=2, sg=1) 
# size : 200차원 벡터
# window : 주변단어는 앞뒤로 10개 씩 분석
# min_count : 출현빈도는 2개 미만 제외
# hs : hs=1 이면 softmax를 트레이닝 할 때 사용, 0 이면 0이 아닌 음수로 샘플링된다.
# sg : 분석방법론으로 CBOW와 Skip-Gram
# CBOW (Continuous Bag of Word) : 주변 단어들을 가지고 중심에 있는 단어를 맞추는 방식
# Skip-Gram : 중심에 있는 단어로 주변단어를 예측하는 방법


model.save("c:/data/yang_w2v.model")

model.most_similar(positive=["춘희"]) # 유사한 값들 추출  most_similar
model.most_similar(positive=["철수"])
model["결혼"]
model["사람"]
model.most_similar(positive=["미술관","여자"] , negative=["여자"]) # 미술관과 여자를 붙였다가 여자를 제외 
model.most_similar(positive=["미술관","여자"]) # 미술관과 여자 연관지어 단어를 뽑을 경우 
model.most_similar(positive=["여자"])

# 단어들의 관계 확률을 알고 싶을 때

model.similarity('춘희','철수')
model.similarity('춘희','인공')







