# binary classification 

import tensorflow as tf


x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]
y_data=  [[0],[0],[0],[1],[1],[1]] # 분류값

# tensor 모양으로 바꾸기

x = tf.placeholder(tf.float32,shape=[None,2])
y = tf.placeholder(tf.float32,shape=[None,1])


w = tf.Variable(tf.random_normal([2,1],seed=0),name='weight')
b = tf.Variable(tf.random_normal([1],seed=0),name='bias')

# afine sum : 여기선 분류를 해야하기 때문에 값이 0 과 1사이로 값이 나와야 하므로 활성화 함수를 사용한다.

hypothesis = tf.sigmoid(tf.matmul(x,w) + b) # 시그모이드 함수를 사용하기 : 0과 1 실수형식

# 문제점 : 오차값과 weight 값을 구할 때 미분을 해야하는데 시그모이드도 미분을 해야한다.
# 시그모이드를 미분하면 골격이 생기게 된다.  즉, 에러가 없다고 나올 수 도 있다.
# 그래서 분류함수 사용시 (로지스틱 회귀 시) 로그함수를 사용한다.

# cost function 
y = 1    -y * log(h(x)) 
y = 0   (1-y) * log(1-h(x))

cost = -tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(hypothesis))

train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)

predict = tf.cast(hypothesis > 0.5, dtype=tf.float32)  # cast T/F 함수와 같다. cast 기준값 >0.5 0.5보다 크면 참
# predict 해석 : if hypothesis > 0.5 then True(1.0) else False(0.0)

accuracy  = tf.reduce_mean(tf.cast(tf.equal(predict,y),dtype=tf.float32)) # 평균을 통해 정확도 측정

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for step in range(10001):
        cost_val,_= sess.run([cost,train],feed_dict={x:x_data,y:y_data})
        if step % 200 == 0:
            print(step,cost_val)
    h,c,a = sess.run([hypothesis,predict,accuracy],feed_dict={x:x_data,y:y_data})
    print('hypothesis : ',h)
    print('predict : ',c)
    print('accuracy : ',a)
    
# 두번을 이용해야한다. 
    
[문제] xor를 logistic regression classifier를 이용해서 프로그램을 생성하세요.

0 0 0
0 1 1
1 0 1
1 1 0

x_data=[[0,0],[0,1],[1,0],[1,1]]
y_data=[[0],[1],[1],[0]]
x_data=np.array(x_data,dtype=np.float32)
y_data=np.array(y_data,dtype=np.float32)

x = tf.placeholder(tf.float32,[None,2])
y = tf.placeholder(tf.float32,[None,1])

# input layer
w1 = tf.Variable(tf.random_normal([2,5],seed=0),name='weight1') # 층을 만들기 위해서 (1,2)로 만들기위해 사용 , 5로 한것은 의도적으로 바꾸기
b1 = tf.Variable(tf.random_normal([5],seed=0),name='bias1')

L1 = tf.sigmoid(tf.matmul(x,w1) + b1) # 첫번째 은닉층

# hidden layer 2

w2 = tf.Variable(tf.random_normal([5,4],seed=0),name='weight2')
b2 = tf.Variable(tf.random_normal([4],seed=0),name='bias2')
L2 = tf.sigmoid(tf.matmul(L1,w2) + b2)

# hidden layer 2

w3 = tf.Variable(tf.random_normal([4,4],seed=0),name='weight3')
b3 = tf.Variable(tf.random_normal([4],seed=0),name='bias3')
L3 = tf.sigmoid(tf.matmul(L2,w3) + b3)

# output layer

w4 = tf.Variable(tf.random_normal([4,1],seed=0),name='weight4')
b4 = tf.Variable(tf.random_normal([1],seed=0),name='bias4')

hypothesis = tf.sigmoid(tf.matmul(L3,w4) + b4)

cost = -tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis)) # cross entropy cost function
train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)

sess=tf.Session()
sess.run(tf.global_variables_initializer())
for step in range(10001):
    sess.run(train,feed_dict={x:x_data,y:y_data})
    if step%1000==0:
        print(step,sess.run(cost,feed_dict={x:x_data,y:y_data}),sess.run([w,w2]))
    h=sess.run(hypothesis,feed_dict={x:x_data,y:y_data})
    print(h)
    
a=sess.run(hypothesis,feed_dict={x:[[0,0],[0,1],[1,0],[1,1]]})
print(sess.run(tf.cast(a>0.5,dtype=tf.int32))) 

# 제대로 해도 나오지 않는다 그렇기 때문에 은닉층을 계속 만들어야한다.
    
############### 선생님꺼 #################
    
import tensorflow as tf
import numpy as np

x_data=[[0,0],[0,1],[1,0],[1,1]]
y_data=[[0],[1],[1],[0]]

x_data=np.array(x_data,dtype=np.float32)
y_data=np.array(y_data,dtype=np.float32)
x=tf.placeholder(tf.float32,[None,2])
y=tf.placeholder(tf.float32,[None,1])

# input
w1=tf.Variable(tf.random_normal([2,5],seed=0),name="weight1") # 원래 [2,2] 인데 나가는거 5로 수정
b1=tf.Variable(tf.random_normal([5],seed=0),name="bias1")
layer1=tf.sigmoid(tf.matmul(x,w1)+b1)

# hidden layer2
w2=tf.Variable(tf.random_normal([5,4],seed=0),name="weight2") # 위에서 5개로 나갔으니가 5개로 받아줌
b2=tf.Variable(tf.random_normal([4],seed=0),name="bias2")
layer2=tf.sigmoid(tf.matmul(layer1,w2)+b2)

# hidden layer3
w3=tf.Variable(tf.random_normal([4,4],seed=0),name="weight3")
b3=tf.Variable(tf.random_normal([4],seed=0),name="bias3")
layer3=tf.sigmoid(tf.matmul(layer2,w3)+b3)

# output layer 
w4=tf.Variable(tf.random_normal([4,1],seed=0),name='weight4')
b4=tf.Variable(tf.random_normal([1],seed=0),name='bias4')
hypothesis=tf.sigmoid(tf.matmul(layer3,w4)+b4)

cost=-tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis)) # cross entropy cost function
train=tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)

sess=tf.Session()
sess.run(tf.global_variables_initializer())
for step in range(10001):
    sess.run(train,feed_dict={x:x_data,y:y_data})
    if step%1000==0:
        print(step,sess.run(cost,feed_dict={x:x_data,y:y_data}),sess.run([w,w2]))
    h=sess.run(hypothesis,feed_dict={x:x_data,y:y_data})
    print(h)

a=sess.run(hypothesis,feed_dict={x:[[0,0],[0,1],[1,0],[1,1]]})
print(sess.run(tf.cast(a>0.5,dtype=tf.int32))) 
  

# train1 으로 데이터 저장 (txt파일) 그리고 실행

'''
#x0,x1,x3,y
1,2,1,0
1,3,2,0
1,3,4,0
1,5,5,1
1,7,5,1
1,2,5,1
1,6,6,2
1,7,7,2
'''

# multi classfication (softmax classifier)  # 확률값으로 표현 ★★

xy = np.loadtxt('c:/data/train1.txt',delimiter=',',dtype=np.float32)
x_data = xy[:,0:-1]
y_data = xy[:,[-1]]

x = tf.placeholder(tf.float32,[None,3])
y = tf.placeholder(tf.int32,[None,1]) 

# y가 0,1,2 로 범주되어있는데 0,1 로 바꿔줘야한다 그래서 one_hot (0과 1) 코드로 바꿔줘야 한다.

y_one_hot = tf.one_hot(y,3) # 0과 1로만 나타내야 하기 때문에 int형으로 해야한다. 독립변수가 3개이므로 3을 적고, 결과 y를 적는다.
y_one_hot = tf.reshape(y_one_hot,[-1,3]) # 3차원으로 적혀있던것이 reshape해서 바뀌어져 있다.

w = tf.Variable(tf.random_normal([3,3]),name='weight') # 분류가 0,1,2 이므로 3개로 한다.
b = tf.Variable(tf.random_normal([3]),name='bias')

logits = tf.matmul(x,w) + b 
hypothesis = tf.nn.softmax(logits) # 확률로 만들어준다.

cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels = y_one_hot)) # one/ hot으로 바꿔서 저장, 확률 값으로 지정
train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost) #

prediction = tf.argmax(hypothesis,1) # 인수 값의 가장 큰값 찾기  1: 차원  2: 1,2차원 , 3: 2차원
                                     # 예측하는 값은 확률이 가장 높은것을 예측하는 것으로 하겠다.

'''TEST
a1 = tf.Variable([0.1,0.3,0.5])
sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run(tf.argmax(a1))) # 가장 높은값 (분류값 중에서)
print(sess.run(tf.argmin(a1))) # 가장 낮은값
sess.close()
'''

correct_prediction = tf.equal(prediction,tf.argmax(y_one_hot,1)) 
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))

sess = tf.Session()
sess.run(tf.global_variables_initializer())
for step in range(2001):
    sess.run(train,feed_dict={x:x_data,y:y_data})
    if step % 100 == 0 : 
        loss,acc = sess.run([cost,accuracy],feed_dict={x:x_data,y:y_data})
        print('step:',step)
        print('loss:',loss) # cost (최저값 찾기)
        print('Acc:', acc)

# 데이터를 훈련시킨 후 가장 높은 값을 추출하는 방향으로 간다.

a = sess.run(hypothesis,feed_dict={x:[[1,2,1]]})
print(a,sess.run(tf.argmax(a,1))) # 0번쪽으로 몰아준다.

b = sess.run(hypothesis,feed_dict={x:[[1,7,7]]})
print(b,sess.run(tf.argmax(b,1))) 

b = sess.run(hypothesis,feed_dict={x:[[1,4,5]]})
print(b,sess.run(tf.argmax(b,1))) 

