# feed forward

입력 -> 출력
     |
     bias

y = w *x + b

w=2
b=1
x
------
0   1
1   3
2   5   

## weight 수정 목표를 찾는다.

목표
입력 1 목표 4
bias 1 

y = w * 1 + b = 4 # w값을 조정해서 목표값을 만들어 주는것
y = 3 * 1 + 1

w
-----------
1       2
2       3
2.5     3.5
3       4

입력 1
weight 2
목표 4

bias 
----------
1       2*1 + 1 =3
1.5     2*1 + 1.5 = 3.5
2       2*1 + 2 = 4

# backpropagation : 역전파 

cost function - 신경망 학습에서 학습데이터에 대한 오차를 측정하는 척도

평균제곱오차(mean squared error, MSE)

오차 = 1/2 * Σ(ytarget - y)² (평균으로 나눈값)     [1/2 * Σ(목표값 - 예측값)²] # bias
오차 = 1/m * Σ(ytarget - y)² (평균으로 나눈값)  # 전체 갯수


import numpy as np
t = np.array([0,0,0,0,1,0,0,0,0,0])
y = np.array([0.1,0.03,0.05,0.2,0.9,0.0,0.1,0.2,0.12,0.03])

t : 타겟값

sum((t-y)**2)/2 # 왜 2로 나누지

def mse(t,y):
    return 0.5*sum((t-y)**2)

print('오차는',mse(t,y))

#  ★★ gradient descentdent method - 경사하강법

∂E
---  # E : 오차 
∂w

                  ∂E 
w 수정 = w - α * ------ # α : learning rate (학습률)
                  ∂w

f = w * x + b
g = w * x
f = g + b

# 입력은 고정이기 때문에 값을 변경하려면 weight값을 조정을 해야한다.

f = wx+b
g = wx

∂g
--- = x 
∂w

∂g
--- = w 
∂x

f = a+b

∂f
--- = 1
∂g


∂f
--- = 1
∂b

# chain rule

∂f    ∂f      ∂g
--- = ---- * ----- = 1 * x
∂w    ∂g      ∂w

∂f    ∂f      ∂g
--- = ---- * ----- = 1 * w
∂x    ∂g      ∂x


linear regression

입력(x) 출력(y)
--------------
1       2
2       4
3       6
4       8
5       10
6       12

7을 입력하면 출력값은?
y = 2*x  - > 14

# 신경망은 타겟에 맞춰서 값을 변환시켜준다.

import tensorflow as tf

x_data = [1,2,3,4,5,6]
y_data = [2,4,6,8,10,12]

x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)
w = tf.Variable(tf.random_normal([1],seed=0), name='weight') # 임의로 계속 변화하므로 변수로 둔다 , 정규분포를 따라서 난수값 지정
b = tf.Variable(tf.random_normal([1],seed=0), name='bias') # 임의의 초기값 지정 , seed로 고정

hypothesis = w*x + b

cost = tf.reduce_mean(tf.square(hypothesis - y)) # 평균제곱오차

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001) # 경사하강법

train = optimizer.minimize(cost) # 가장 작은값 추출

sess = tf.Session()
sess.run(tf.global_variables_initializer()) # 변수는 초기화를 시켜야 가능

# 학습을 시킨다.

for step in range(2001):
    cost_v,w_v,b_v,_ = sess.run([cost,w,b,train],feed_dict={x:x_data,y:y_data})# train 값은 받을 값이 없어서 _ ?
    if step % 20 == 0 : # 일부값만 print
        print(step,cost_v,w_v,b_v)

print(sess.run(hypothesis , feed_dict={x:7})) # 예측값 확인

기울기 = 4.3
절편 = 64

공부시간    점수
---------------
2           71
4           83
6           91
8           97

ab = [4.3,64]
data = [[2,71],[4,83],[6,91],[8,97]]

x = np.array(data)[:,0]
y = np.array(data)[:,1]

x1 = [i for i,j in data]
y1 = [j for i,j in data]

def predict(x):
    return ab[0] * x + ab[1]

predict(2)

RMSE(Root Mean Squared Error) # 평균제곱오차 

def rmse(p,a):
    return np.sqrt(((p-a)**2).mean())

def rmse_val(predict_result,y):
    return rmse(np.array(predict_result),np.array(y))

predict_result = []
for i in range(len(x)):
    predict_result.append(predict(x[i]))
    print('공부시간 : %.f, 실제점수 : %.f, 예측점수 : %.f' %(x[i],y[i],predict(x[i])))

print('오차 : ',rmse_val(predict_result,y))

# 선형회귀 - 임의의 직선을 그어 이에 대한 평균제곱근오차를 구하고 이 값을 가장 작게 만들어 주는 기울기와 절편을 찾아가는 작업

# 신경망으로 기울기, bias 구하기

ab = [4.3,64] # 여기선 모른다.
data = [[2,71],[4,83],[6,91],[8,97]]
x1 = [i for i,j in data]
y1 = [j for i,j in data]

x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)
w = tf.Variable(tf.random_normal([1],seed=0), name='weight') # 임의로 계속 변화하므로 변수로 둔다 , 정규분포를 따라서 난수값 지정
b = tf.Variable(tf.random_normal([1],seed=0), name='bias') # 임의의 초기값 지정 , seed로 고정

hypothesis = w*x + b

cost = tf.reduce_mean(tf.square(hypothesis - y)) # 평균제곱오차

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) # 경사하강법

train = optimizer.minimize(cost) # 가장 작은값 추출

sess = tf.Session()
sess.run(tf.global_variables_initializer()) # 변수는 초기화를 시켜야 가능

# 학습을 시킨다.

for step in range(2001):
    cost_v,w_v,b_v,_ = sess.run([cost,w,b,train],feed_dict={x:x1,y:y1})# train 값은 받을 값이 없어서 _ ?
    if step % 20 == 0 : # 일부값만 print
        print(step,cost_v,w_v,b_v)

w_v
b_v

print(sess.run(hypothesis , feed_dict={x:x1})) # 예측값 확인
y1

# 다른 방법, 1/2로 했을 경우 

x1 = [i for i,j in data]
y1 = [j for i,j in data]

x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)
w = tf.Variable(tf.random_normal([1],seed=0), name='weight') # 임의로 계속 변화하므로 변수로 둔다 , 정규분포를 따라서 난수값 지정
b = tf.Variable(tf.random_normal([1],seed=0), name='bias') # 임의의 초기값 지정 , seed로 고정

hypothesis = w*x + b

cost1 = 1/2*tf.reduce_sum(tf.square(hypothesis - y)) # 평균제곱오차
optimizer1 = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train1 = optimizer1.minimize(cost1)

sess = tf.Session()
sess.run(tf.global_variables_initializer()) # 변수는 초기화를 시켜야 가능

for step in range(2001):
    cost_v,w_v,b_v,_ = sess.run([cost1,w,b,train1],feed_dict={x:x1,y:y1})# train 값은 받을 값이 없어서 _ ?
    if step % 20 == 0 : # 일부값만 print
        print(step,cost_v,w_v,b_v)

w_v
b_v
print(sess.run(hypothesis , feed_dict={x:x1}))
y1

# 회귀분석으로 기울기 오차 구해보기

from scipy import stats

slope, intercept, r_value, p_value, stderr = stats.linregress(x1,y1) 

slope
intercept

# rmse로 구하기, 평균제곱근 오차

x1 = [i for i,j in data]
y1 = [j for i,j in data]

x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)
w = tf.Variable(tf.random_normal([1],seed=0), name='weight') # 임의로 계속 변화하므로 변수로 둔다 , 정규분포를 따라서 난수값 지정
b = tf.Variable(tf.random_normal([1],seed=0), name='bias') # 임의의 초기값 지정 , seed로 고정

hypothesis = w*x + b

rmse = tf.sqrt(tf.reduce_mean(tf.square(hypothesis - y))) # 평균제곱오차
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train = optimizer.minimize(rmse)

sess = tf.Session()
sess.run(tf.global_variables_initializer()) # 변수는 초기화를 시켜야 가능

for step in range(2001):
    cost_v,w_v,b_v,_ = sess.run([rmse,w,b,train],feed_dict={x:x1,y:y1})# train 값은 받을 값이 없어서 _ ?
    if step % 20 == 0 : # 일부값만 print
        print(step,cost_v,w_v,b_v)

cost_v # 오차가 비슷함을 알 수 있다.

문제 linear regression 학습을 통해서 입력값에 대한 예측값을 출력하세요

x1 x2 x3   y
-------------
73 80 75  152
93 88 93  185
89 91 90  180
96 98 100 196
73 66 70  142


x1_data=[73, 93, 89, 96, 73]
x2_data=[80, 88, 91, 98, 66]
x3_data=[75,93,90,100,70]


x1 = tf.placeholder(tf.float32)
x2 = tf.placeholder(tf.float32)
x3 = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)
w1 = tf.Variable(tf.random_normal([1],seed=0), name='weight1')
w2 = tf.Variable(tf.random_normal([1],seed=0), name='weight2')
w3 = tf.Variable(tf.random_normal([1],seed=0), name='weight3')
b = tf.Variable(tf.random_normal([1],seed=0), name='bias') 

hypothesis = w1*x1 + w2*x2 + w3*x3 + b

cost = tf.reduce_mean(tf.square(hypothesis - y)) # 평균제곱오차
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.00001) # 경사하강법
train = optimizer.minimize(cost) # 가장 작은값 추출

sess = tf.Session()
sess.run(tf.global_variables_initializer())


for step in range(10001):
    cost_v,w_1,w_2,w_3,b_v,train_v= sess.run([cost,w1,w2,w3,b,train],feed_dict={x1:x1_data,x2:x2_data,x3:x3_data,y:y_data})
    if step % 100 == 0 : 
        print(cost_v,w_1,w_2,w_3,b_v)

w_1
w_2
w_3
print('당신의 점수는 ',sess.run(hypothesis , feed_dict={x1:100,x2:70,x3:60}))

# 변수가 많을 때 잘 활용하는 방법

x_data = [[73,80,75],[93,88,93],[89,91,90],[96,98,100],[73,66,70]]
y_data = [[152],[185],[180],[196],[142]]

x = tf.placeholder(tf.float32,shape=[None,3]) # None 은 몇개가 들어올지 모르기 때문에 None으로 진행한다.
y = tf.placeholder(tf.float32,shape=[None,1]) # y는 실질적 결과이므로 1로 둔다.

w = tf.Variable(tf.random_normal([3,1],seed=0),name='weight') # x와 값을 맞춰야 한다.
b = tf.Variable(tf.random_normal([1],seed=0),name='bias')
# [3,1] 3: x의(열값) 들어오는 값, 1은 y로 나가는 값이다.
# 1: bias 1은 y로 나가는 값을 의미 한다.

hypothesis = tf.matmul(x,w)+b # 행렬이기 때문에 matmul을 통해 값을 집어 넣는다.

cost = tf.reduce_mean(tf.square(hypothesis - y)) # 평균제곱오차
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.00001) # 경사하강법
train = optimizer.minimize(cost) # 가장 작은값 추출
sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(10001):
    cost_val,hy_val,train_v= sess.run([cost,hypothesis,train],feed_dict={x:x_data,y:y_data})
    if step % 100 == 0 : 
        print(step,cost_val,hy_val)
        
#### 값이 파일로 있을 때 불러와서 해결하는 방법 #파일은 모든파일로 형식을 바꾸고 csv , ANSI 파일로 저장

import tensorflow as tf
import numpy as np

# loadtxt : 파일을 불러온다 (delimiter - 구분자, dtype - file type) 

xy = np.loadtxt('c:/data/ex.csv',delimiter=',',dtype=np.float32) 

x_data=xy[:,0:-1]
y_data=xy[:,[-1]] # xy[:,-1] 로 뽑지 않은 이유 : 1행 5열로 만들어지기 때문에 x와 방향을 맞게 해야 하기 때문이다.
x_data.shape
y_data.shape

x = tf.placeholder(tf.float32,shape=[None,3]) # None 은 몇개가 들어올지 모르기 때문에 None으로 진행한다.
y = tf.placeholder(tf.float32,shape=[None,1]) # y는 실질적 결과이므로 1로 둔다.

w = tf.Variable(tf.random_normal([3,1],seed=0),name='weight')
b = tf.Variable(tf.random_normal([1],seed=0),name='bias')
        
hypothesis = tf.matmul(x,w)+b # 행렬이기 때문에 matmul을 통해 값을 집어 넣는다.

cost = tf.reduce_mean(tf.square(hypothesis - y)) # 평균제곱오차
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.00001) # 경사하강법
train = optimizer.minimize(cost) # 가장 작은값 추출
sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(10001):
    cost_val,hy_val,train_v= sess.run([cost,hypothesis,train],feed_dict={x:x_data,y:y_data})
    if step % 100 == 0 : 
        print(step,cost_val,hy_val)

print('당신의 점수는' , sess.run(hypothesis, feed_dict={x:[[100,70,60]]}))
print('당신의 점수는' , sess.run(hypothesis, feed_dict={x:[[100,70,60],[90,100,80]]}))

